{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Modules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Environment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from gym.envs.toy_text import frozen_lake, discrete\n",
    "from gym.envs.registration import register\n",
    "\n",
    "\n",
    "register(\n",
    "    id='Deterministic-4x4-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': False})\n",
    "\n",
    "register(\n",
    "    id='Deterministic-8x8-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '8x8',\n",
    "            'is_slippery': False})\n",
    "\n",
    "register(\n",
    "    id='Stochastic-4x4-FrozenLake-v0',\n",
    "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "    kwargs={'map_name': '4x4',\n",
    "            'is_slippery': True})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Evaluation function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, max_iteration=100, tol=1e-3):\n",
    "    \"\"\"Evaluate the value function from a given policy.\n",
    "    Parameters\n",
    "    ----------\n",
    "    P: dictionary\n",
    "        It is from gym.core.Environment\n",
    "        P[state][action] is tuples with (probability, nextstate, reward, terminal)\n",
    "    nS: int\n",
    "        number of states\n",
    "    nA: int\n",
    "        number of actions\n",
    "    gamma: float\n",
    "        Discount factor. Number in range [0, 1)\n",
    "    policy: np.array\n",
    "        The policy to evaluate. Maps states to actions.\n",
    "    max_iteration: int\n",
    "        The maximum number of iterations to run before stopping. Feel free to change it.\n",
    "    tol: float\n",
    "        Determines when value function has converged.\n",
    "    Returns\n",
    "    -------\n",
    "    value function: np.ndarray\n",
    "    The value function from the given policy.\n",
    "    \"\"\"\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    value_function = np.zeros(nS)\n",
    "    new_value_function = value_function.copy()\n",
    "    i = 0\n",
    "    while i<=max_iteration or np.sum(np.sqrt(np.square(new_value_function-value_function)))>tol:\n",
    "        i += 1\n",
    "        value_function = new_value_function.copy()\n",
    "        for state in range(nS):\n",
    "            result = P[state][policy[state]]\n",
    "            new_value_function[state] = np.array(result)[:,2].mean()\n",
    "            for num in range(len(result)):\n",
    "                (probability, nextstate, reward, terminal) = result[num]\n",
    "                new_value_function[state] += (gamma * probability * value_function[nextstate])\n",
    "    ############################\n",
    "    return new_value_function"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    updated_value_function = np.zeros(nS)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    while True:\n",
    "        for state in range(nS):\n",
    "            for next_state in P[state][policy[state]]:\n",
    "                probability = next_state[0]\n",
    "                new_state = next_state[1]\n",
    "                reward = next_state[2]/len(P[state][policy[state]])\n",
    "                #print(gamma*probability*value_function[new_state])\n",
    "                updated_value_function[state] += (reward + gamma*probability*value_function[new_state])\n",
    "                \n",
    "        diff = max(np.abs(updated_value_function - value_function))\n",
    "        if diff <= tol:\n",
    "            break\n",
    "        else:\n",
    "            value_function = updated_value_function.copy()\n",
    "            updated_value_function = np.zeros(nS)\n",
    "            #print(updated_value_function, '\\n')\n",
    "            #print(value_function)\n",
    "\n",
    "    ############################\n",
    "    return value_function"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Improvement function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "\n",
    "\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    for state in range(nS):\n",
    "        actions = []\n",
    "        for action in range(nA):\n",
    "            value = 0\n",
    "            for next_state in P[state][action]:\n",
    "                probability = next_state[0]\n",
    "                new_state = next_state[1]\n",
    "                reward = next_state[2]/len(P[state][action])\n",
    "                value += (reward + gamma*probability*value_from_policy[new_state])\n",
    "            actions.append(value)\n",
    "        new_policy[state] = np.argmax(actions)\n",
    "            \n",
    "    ############################\n",
    "    return new_policy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy Iteration using Dynamic Programming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    for i in range(10):\n",
    "        value_from_policy = policy_evaluation(P, nS, nA, policy, gamma, tol)\n",
    "        policy = policy_improvement(P, nS, nA, value_from_policy, policy, gamma)\n",
    "        #policy = new_policy.copy()\n",
    "        #diff = max(np.abs(value_from_policy - value_function))\n",
    "        '''if diff <= tol:\n",
    "            break\n",
    "        else:\n",
    "            value_function = value_from_policy.copy()'''\n",
    "\n",
    "\n",
    "    ############################\n",
    "    return value_from_policy, policy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Value Iteration using Dynamic Promgramming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
    "\n",
    "    value_function = np.zeros(nS)\n",
    "    updated_value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    count = 0\n",
    "    ############################\n",
    "    # YOUR IMPLEMENTATION HERE #\n",
    "    while True:\n",
    "        count += 1\n",
    "        for state in range(nS):\n",
    "            for next_state in P[state][policy[state]]:\n",
    "                probability = next_state[0]\n",
    "                new_state = next_state[1]\n",
    "                reward = next_state[2]/len(P[state][policy[state]])\n",
    "                #print(gamma*probability*value_function[new_state])\n",
    "                updated_value_function[state] += (reward + gamma*probability*value_function[new_state])\n",
    "                \n",
    "        policy = policy_improvement(P, nS, nA, value_function, policy, gamma)\n",
    "        \n",
    "        diff = max(np.abs(updated_value_function - value_function))\n",
    "        if diff <= tol and count>10:\n",
    "            break\n",
    "        else:\n",
    "            value_function = updated_value_function.copy()\n",
    "            updated_value_function = np.zeros(nS)\n",
    "            #print(updated_value_function, '\\n')\n",
    "            #print(value_function)\n",
    "\n",
    "\n",
    "    ############################\n",
    "    return value_function, policy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output for Policy Iteration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "  \tprint(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n",
    "# Edit below to run policy and value iteration on different environments and\n",
    "# visualize the resulting policies in action!\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "\tenv = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "\t#env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "\tprint(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "\tV_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-6)\n",
    "\trender_single(env, p_pi, 100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Output for Value Iteration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "  \tprint(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n",
    "# Edit below to run policy and value iteration on different environments and\n",
    "# visualize the resulting policies in action!\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "\tenv = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "\t# env = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "\tprint(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "\tV_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
    "\trender_single(env, p_vi, 100)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Value Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic Environment Setting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def render_single(env, policy, max_steps=100):\n",
    "  \"\"\"\n",
    "    This function does not need to be modified\n",
    "    Renders policy once on environment. Watch your agent play!\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.core.Environment\n",
    "      Environment to play on. Must have nS, nA, and P as\n",
    "      attributes.\n",
    "    Policy: np.array of shape [env.nS]\n",
    "      The action to take at a given state\n",
    "  \"\"\"\n",
    "\n",
    "  episode_reward = 0\n",
    "  ob = env.reset()\n",
    "  for t in range(max_steps):\n",
    "    env.render()\n",
    "    time.sleep(0.25)\n",
    "    a = policy[ob]\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    episode_reward += rew\n",
    "    if done:\n",
    "      break\n",
    "  env.render();\n",
    "  if not done:\n",
    "    print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
    "  else:\n",
    "  \tprint(\"Episode reward: %f\" % episode_reward)\n",
    "\n",
    "\n",
    "# Edit below to run policy and value iteration on different environments and\n",
    "# visualize the resulting policies in action!\n",
    "# You may change the parameters in the functions below\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\t# comment/uncomment these lines to switch between deterministic/stochastic environments\n",
    "\t#env = gym.make(\"Deterministic-4x4-FrozenLake-v0\")\n",
    "\tenv = gym.make(\"Stochastic-4x4-FrozenLake-v0\")\n",
    "\n",
    "\tprint(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
    "\n",
    "\tV_pi, p_pi = policy_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-6)\n",
    "\trender_single(env, p_pi, 100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "-------------------------\n",
      "Beginning Policy Iteration\n",
      "-------------------------\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "Episode reward: 1.000000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('virenv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "ed0139f148f2b63079f0b3b72fc2f5d8abd5a73cc3fcdc03b45aa74ae32df9a4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}